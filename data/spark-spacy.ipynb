{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Goals\n",
    "\n",
    "### - Get to know Apache Spark engine.\n",
    "\n",
    "### - Understand Spacy NLP library capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark is a fast and general engine for large-scale data processing\n",
    "![Spark Libs](img/spark-libs.png)\n",
    "\n",
    "### It can access diverse data sources including HDFS, Cassandra, Hive, HBase, S3 and JDBC/ODBC\n",
    "![Spark Compatabilities](img/spark-cmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hadoop data sharing](img/data-sharing-mapreduce.png)\n",
    "![Spark data sharing](img/data-sharing-spark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun, types\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(SparkContext.getOrCreate()) \\\n",
    "    .builder \\\n",
    "    .appName('NLP') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Category Dataset:\n",
    "https://www.kaggle.com/rmisra/news-category-dataset\n",
    "\n",
    "Each json record contains following attributes:\n",
    "\n",
    "* category: Category article belongs to\n",
    "\n",
    "* headline: Headline of the article\n",
    "\n",
    "* authors: Person authored the article\n",
    "\n",
    "* link: Link to the post\n",
    "\n",
    "* short_description: Short description of the article\n",
    "\n",
    "* date: Date the article was published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = spark.read.json(\"News_Category_Dataset_v2.json\")\n",
    "news_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.createOrReplaceTempView(\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS count FROM news\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT category, count(category) AS count FROM news GROUP BY category ORDER BY count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.groupby('category') \\\n",
    "    .count() \\\n",
    "    .orderBy(fun.desc('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Select the longest headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use `length` function and `LIMIT` expression in SQL\n",
    "\n",
    "Available functions: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy NLP library\n",
    "![Spacy Features](img/spacy-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy pipeline\n",
    "![Spacy Features](img/spacy-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"In 2018 the Debian Linux project received a donation of $300,000\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc.noun_chunks:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Extract named entities from the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use `ents` attribute of the `Doc` and `label_` attribute of the `Token`\n",
    "\n",
    "Spacy Cheat Sheet: http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine a power of these two instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Extract ORG, PERSON, GPE named entities in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Write a function that takes a news headline and generate the output like that\n",
    "\n",
    "[\n",
    "  {\n",
    "    'label': 'ORG', \n",
    "    'text': 'ACME Inc.'\n",
    "  },\n",
    "  {\n",
    "    'label': 'PERSON', \n",
    "    'text': 'John Doe'   \n",
    "  },\n",
    "  {\n",
    "    'label': GPE,\n",
    "    'text': 'London'\n",
    "  }\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df_sample = news_df.sample(withReplacement=False, fraction=0.002, seed=777)\n",
    "news_df_sample.createOrReplaceTempView(\"news_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyWrapper(object):\n",
    "    \"\"\"Wrapper class to load Spacy on worker nodes\"\"\"\n",
    "    _spacys = {}\n",
    "    disabled_pipeline_steps = ['parser', 'tagger']\n",
    "    default_model = 'en_core_web_sm'\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, model=default_model, disable=disabled_pipeline_steps):\n",
    "        if model not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[model] = spacy.load(model, disable=disable)\n",
    "        return cls._spacys[model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neamed entity extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Reuse the code from `Task 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(doc):\n",
    "    labels=['ORG', 'PERSON', 'GPE']\n",
    "    entities = []\n",
    "    \n",
    "    # Load Spacy\n",
    "    nlp = SpacyWrapper.get()\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    # ======== WRITE YOUR SOLUTION BELOW ======== \n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "schema = types.ArrayType(\n",
    "    types.StructType([\n",
    "        types.StructField('label', types.StringType(), nullable=False),\n",
    "        types.StructField('text', types.StringType(), nullable=False)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Register user defined function (UDF) to use in SQL\n",
    "spark.udf.register('ner', ner, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply UDF to extract headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sample = spark.sql(\"SELECT short_description, ner(short_description) AS entities FROM news_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sample.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT short_description, ner(short_description) AS entities FROM news_sample\") \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .json(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier box\n",
    "\n",
    "model = spacy.blank(\"en\")\n",
    "textcat = model.create_pipe(\n",
    "    \"textcat\",\n",
    "    config={\n",
    "        \"exclusive_classes\": True,\n",
    "        \"architecture\": \"simple_cnn\",\n",
    "    }\n",
    ")\n",
    "model.add_pipe(textcat, last=True)\n",
    "\n",
    "# Get labels & propagate them into the model\n",
    "labels = [row.category for row in news_df.select(news_df.category).distinct().collect()]\n",
    "for lbl in labels:\n",
    "    textcat.add_label(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data box\n",
    "\n",
    "from pyspark.sql.types import MapType, StringType, BooleanType\n",
    "\n",
    "SEED = 15\n",
    "\n",
    "# Splpit data\n",
    "train_df, test_df = news_df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "\n",
    "# Broadcast labels to all workers\n",
    "wlabels = spark.sparkContext.broadcast(labels)\n",
    "\n",
    "# Create udf to for label to cats conversion\n",
    "fschema = MapType(StringType(), BooleanType(), False)\n",
    "def tocats(label):\n",
    "    return {lbl: (lbl == label) for lbl in wlabels.value}\n",
    "spark.udf.register('tocats', tocats, fschema)\n",
    "\n",
    "train_df.createOrReplaceTempView(\"train_df\")\n",
    "test_df.createOrReplaceTempView(\"test_df\")\n",
    "\n",
    "prepared_df = spark.sql(\"SELECT concat_ws('. ', headline, short_description) as text, tocats(category) as cats from train_df\")\n",
    "\n",
    "# Get dataset on the driver for training\n",
    "train_data = [(row.text, {\"cats\": row.cats}) for row in prepared_df.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model box\n",
    "\n",
    "import random\n",
    "import os\n",
    "from shutil import rmtree\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "ITERATIONS = 10\n",
    "\n",
    "print(\"{:^5}\".format(\"LOSS\"))\n",
    "\n",
    "optimizer = model.begin_training()\n",
    "batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_sizes)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        model.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "    print(\"{0:.3f}\".format(losses[\"textcat\"]))\n",
    "\n",
    "# Save the model to disk for the workers to use it\n",
    "MODEL_DIR = \"%s/classifier\" % (os.getcwd())\n",
    "\n",
    "# Clean existing and save\n",
    "rmtree(MODEL_DIR, ignore_errors=True)\n",
    "with model.use_params(optimizer.averages):\n",
    "    model.to_disk(MODEL_DIR)\n",
    "print(\"Saved model to\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classifier box\n",
    "from operator import itemgetter\n",
    "\n",
    "class SpacyWrapper(object):\n",
    "    \"\"\"Wrapper class to load Spacy on worker nodes\"\"\"\n",
    "    _spacys = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, model_dir):\n",
    "        if model_dir not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[model_dir] = spacy.load(model_dir)\n",
    "        return cls._spacys[model_dir]\n",
    "\n",
    "def predict_category(text):\n",
    "    model = SpacyWrapper.get(MODEL_DIR)\n",
    "    res = model(text)\n",
    "    # return the category with the highest prediction\n",
    "    return sorted(res.cats.items(), key=itemgetter(1), reverse=True)[0][0]\n",
    "spark.udf.register(\"predict_category\", predict_category, StringType())\n",
    "\n",
    "predicted_results = spark.sql(\"select category, predict_category(concat_ws('. ', headline, short_description)) as prediction from test_df\")\n",
    "\n",
    "total_tested = test_df.count()\n",
    "correct_prediction = predicted_results.filter(predicted_results.category == predicted_results.prediction).count()\n",
    "correctness = correct_prediction/total_tested\n",
    "uniform_labels = 1/len(labels)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"%s total in test, %s predicted correctly, %.3f\" % (total_tested, correct_prediction, correctness))\n",
    "print(\"Prediction by guessing categories as uniformly distributed: %.3f\" % uniform_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
